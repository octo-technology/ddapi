{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datadriver for DataScientists - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Execute the following cell in order to make the table of contents appear_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will discover how you can get your workflow from the notebook to the airflow interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow Context\n",
    "We covered in the previous notebook how you can create a context that allows you to create datasets and models. In this part, you'll learn how to use a new context, the AirflowContext, to create a dataflow and push it to Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Airflow](https://github.com/apache/incubator-airflow) is a workflow management platform developed at AirBnb. It uses the concept of DAGs (Direct Acyclic Graphs) to schedule tasks, which are called operator. If you were to write some vanilla airflow code, you would then write, in python (from the airflow tutorial):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2015, 6, 1),\n",
    "    'email': ['airflow@airflow.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG('tutorial', default_args=default_args)\n",
    "\n",
    "# t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "t1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag)\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='sleep',\n",
    "    bash_command='sleep 5',\n",
    "    retries=3,\n",
    "    dag=dag)\n",
    "\n",
    "t2.set_upstream(t1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you must create a dag, tasks, bind tasks to the DAG (through their constructors), and link tasks together in order to create the dependency graph. It is a really powerful approach, but a bit verbose and tedious (and imperative). This is where the airflow context comes into play to save the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new context to bind them all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will rewrite here the same code as we used in the previous notebook, removing all the unnecessary stuff and keeping the most important parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dd import DB\n",
    "from dd.api.contexts import LocalContext\n",
    "import pkg_resources\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Context\n",
    "db = DB(dbtype='sqlite', filename=':memory:')\n",
    "context = LocalContext(db)\n",
    "\n",
    "# Loading data\n",
    "titanic_datapath = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\n",
    "train = context.load_file(titanic_datapath,\n",
    "                          table_name=\"titanic.train\",\n",
    "                          write_options=dict(if_exists=\"replace\", index=False))\n",
    "\n",
    "# Feature engineering\n",
    "def fillna_with_zeros(dataframe):\n",
    "    \"\"\"\n",
    "    Returns a copy of the dataframe with null values replaced by zeros.\n",
    "    \"\"\"\n",
    "    return dataframe.fillna(0)\n",
    "\n",
    "filled_with_zeros = train.transform(fillna_with_zeros)\n",
    "some_columns = filled_with_zeros[[\"passengerid\", \"survived\", \"pclass\", \"age\", \"sibsp\", \"parch\", \"fare\"]]\n",
    "Xtrain, Xtest = some_columns.split_train_test(train_size=0.75)\n",
    "\n",
    "# Model\n",
    "scikit_model = RandomForestClassifier(max_depth=4, n_jobs=-1) \n",
    "model = context.model(scikit_model, model_address=\"model@foo.bar\")\n",
    "fitted_model = model.fit(Xtrain, target=\"survived\")\n",
    "\n",
    "# Predictions\n",
    "predictions = fitted_model.predict(Xtest, target=\"survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is all the code you need in order to start making predictions. Now, if you were to create a dataflow that can be used by airflow, here is the new code you should write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dd import DB  # <- new import\n",
    "from dd.api.contexts import AirflowContext  # <- new import\n",
    "import pkg_resources\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "from airflow import DAG\n",
    "\n",
    "# Context\n",
    "db = DB(dbtype='sqlite', filename=':memory:')\n",
    "dataflow = DAG(\"my_first_dataflow\", start_date=datetime.now())  # <- creation of an empty dataflow\n",
    "context = AirflowContext(dataflow, db)  # <- new context\n",
    "context.set_default_write_options(if_exists=\"replace\", index=False)\n",
    "\n",
    "# Loading data\n",
    "titanic_datapath = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\n",
    "train = context.load_file(titanic_datapath,\n",
    "                          table_name=\"titanic.train\",\n",
    "                          write_options=dict(if_exists=\"replace\", index=False))\n",
    "\n",
    "# Feature engineering\n",
    "def fillna_with_zeros(dataframe):\n",
    "    \"\"\"\n",
    "    Returns a copy of the dataframe with null values replaced by zeros.\n",
    "    \"\"\"\n",
    "    return dataframe.fillna(0)\n",
    "\n",
    "filled_with_zeros = train.transform(fillna_with_zeros)\n",
    "some_columns = filled_with_zeros[[\"passengerid\", \"survived\", \"pclass\", \"age\", \"sibsp\", \"parch\", \"fare\"]]\n",
    "Xtrain, Xtest = some_columns.split_train_test(train_size=0.75)\n",
    "\n",
    "# Model\n",
    "scikit_model = RandomForestClassifier(max_depth=4, n_jobs=-1)\n",
    "try :\n",
    "    db.drop_table('foo.bar')\n",
    "except Exception as e :\n",
    "    print(e)\n",
    "    \n",
    "model = context.model(scikit_model, model_address=\"model@foo.bar\")\n",
    "fitted_model = model.fit(Xtrain, target=\"survived\")\n",
    "\n",
    "\n",
    "# Predictions\n",
    "predictions = fitted_model.predict(Xtest, target=\"survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the difference ? A grand total of 4 lines, including imports ! Well, that's neat. All you did was basically creating a empty dataflow and feeding it to a new object, the AirflowContext. And you're done. You don't believe me ? Well, first, let's check if we can still make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW, that's a lot of information right there ! \n",
    "\n",
    "Well, yes, and it should be that way. Airflow is a production platform and as such it requires a decent level of logging. What you see just above is the result of all the intermediary computation, from loading the data to the production of the final predictions. That's why you should probably use the LocalContext when playing around with your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I showed you that using this new context does the same thing, but adds more verbosity. So what's all the hype for ? Well, it does a bit more that adding logs. It also populated your dataflow : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataflow.task_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, your dataflow is not empty anymore. It now has 7 distinct tasks that are ready to be pushed to production. Which means that if you copy this code into a file where airflow can find it, then your code will be handle by it. We know copy-pasting code is not fun, so we created a shortcut for it. Just click on the Runtools button, and on the rocket. Then, after a bit, you should see this in the [Airflow UI](http://localhost:8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TODO : add a screenshot_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
